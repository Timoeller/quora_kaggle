{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timomoeller/.virtualenvs/keras/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 6)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/timomoeller/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timomoeller/.virtualenvs/keras/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "%matplotlib inline\n",
    "from keras.utils import np_utils\n",
    "\n",
    "train = pd.read_csv('../train.csv', sep=',', header=0)\n",
    "#test = pd.read_csv('../test.csv', sep=',', header=0)\n",
    "\n",
    "train = train.fillna(\" \")\n",
    "#test = test.fillna(\" \")\n",
    "print(train.shape)\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/timomoeller/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "import difflib\n",
    "from collections import Counter\n",
    "\n",
    "def diff_ratios(st1, st2):\n",
    "    seq = difflib.SequenceMatcher()\n",
    "    seq.set_seqs(str(st1).lower(), str(st2).lower())\n",
    "    return seq.ratio()\n",
    "\n",
    "def noun_extracter(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    return np.array([w for w, t in nltk.pos_tag(tokens) if t[:1] in ['N']])\n",
    "    \n",
    "def word_match_share(q1,q2):\n",
    "    q1 = q1.split()\n",
    "    q2 = q2.split()\n",
    "    if(len(q1) == 0 or len(q2) == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        both = set(q1).intersection(set(q2))\n",
    "        return len(both) * 2 / (len(q1) + len(q2))\n",
    "    \n",
    "def removePunktAndStopwords(phrases):\n",
    "    toInclude = set(['above','below','between','but','couldn','didn','doesn','down',\n",
    "                     'few','hadn','haven','isn','just','mightn','mustn','needn','nor',\n",
    "                     'not','off','once','only','out','over','should','shouldn','some',\n",
    "                     'very','wasn','weren','won','wouldn','again','against','all',\n",
    "                     'any','aren'])\n",
    "    stop_wordsLarge = set(stopwords.words('english'))\n",
    "    stop_words = set([w for w in stop_wordsLarge if w not in toInclude])\n",
    "    tokenizer = RegexpTokenizer('[a-z]\\w+')\n",
    "    docs_toke = [tokenizer.tokenize(doc.lower()) for doc in phrases]\n",
    "    stopwordsremoved = [[t for t in temp if t not in stop_words] for temp in docs_toke]\n",
    "    return np.asarray([\" \".join(w) for w in stopwordsremoved])\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.snbstem = SnowballStemmer('english')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stop_words.remove('but')\n",
    "        self.stop_words.remove('not') # left outer join would be nice...\n",
    "        self.tokenizer = RegexpTokenizer('[a-z]\\w+')\n",
    "    def lmtokenize(self,doc):\n",
    "        temp = [self.snbstem.stem(t) for t in self.tokenizer.tokenize(doc.lower()) if t not in self.stop_words]\n",
    "        return \" \".join(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timomoeller/.virtualenvs/keras/lib/python3.5/site-packages/ipykernel/__main__.py:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "def getFeatures(df):\n",
    "    questions1 = df[\"question1\"].values\n",
    "    questions2 = df[\"question2\"].values\n",
    "\n",
    "    stopWRe_q1 = removePunktAndStopwords(questions1)\n",
    "    stopWRe_q2 = removePunktAndStopwords(questions2)\n",
    "\n",
    "    df['question1_nouns'] = pd.Series(Parallel(n_jobs=7)(delayed(noun_extracter)(x) for x in questions1))\n",
    "    df['question2_nouns'] = pd.Series(Parallel(n_jobs=7)(delayed(noun_extracter)(x) for x in questions2))\n",
    "    df['z_len1'] = df.question1.map(lambda x: len(str(x)))\n",
    "    df['z_len2'] = df.question2.map(lambda x: len(str(x)))\n",
    "    df['z_word_len1'] = df.question1.map(lambda x: len(str(x).split()))\n",
    "    df['z_word_len2'] = df.question2.map(lambda x: len(str(x).split()))\n",
    "    df['z_noun_match'] = df.apply(lambda r: \n",
    "                                        sum([1 for w in r.question1_nouns if w in r.question2_nouns]), axis=1)\n",
    "    df['z_match_ratio'] = pd.Series(Parallel(n_jobs=7)\n",
    "                                       (delayed(diff_ratios)(x,y) for x,y in zip(stopWRe_q1,stopWRe_q2)))\n",
    "    df['z_word_match'] = pd.Series(Parallel(n_jobs=7)\n",
    "                                      (delayed(word_match_share)(x,y) for x,y in zip(questions1,questions2)))\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "train = getFeatures(train)\n",
    "import pickle\n",
    "pickle.dump(train,open( \"trainFeatures.p\", \"wb\" ))\n",
    "#test = getFeatures(test)\n",
    "col = [c for c in train.columns if c[:1]=='z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessor = LemmaTokenizer()\n",
    "stemmed_q1 = Parallel(n_jobs=6)(delayed(preprocessor.lmtokenize)(sentence) for sentence in stopWRe_q1)\n",
    "stemmed_q2 = Parallel(n_jobs=6)(delayed(preprocessor.lmtokenize)(sentence) for sentence in stopWRe_q2)\n",
    "\n",
    "length1 = np.array([len(x.split()) for x in stopWRe_q1])\n",
    "length2 = np.array([len(x.split()) for x in stopWRe_q2])\n",
    "\n",
    "vect = CountVectorizer(max_df=0.6,min_df=4)\n",
    "vect.fit(np.concatenate((stemmed_q1,stemmed_q2)))\n",
    "bow_q1 = vect.transform(stemmed_q1)\n",
    "bow_q2 = vect.transform(stemmed_q2)\n",
    "vocab_q1 = np.asarray([\"q1_\" + x for x in vect.get_feature_names()])\n",
    "vocab_q2 = np.asarray([\"q2_\" + x for x in vect.get_feature_names()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 46186)\n",
      "done preprocessing\n"
     ]
    }
   ],
   "source": [
    "features = sp.sparse.hstack([bow_q1,bow_q2,train[col].values])\n",
    "labels = train[\"is_duplicate\"]\n",
    "feature_names = np.concatenate([vocab_q1,vocab_q2,np.array(col)])\n",
    "print(features.shape)\n",
    "print(\"done preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(780486, 15)\n",
      "(404290, 15)\n"
     ]
    }
   ],
   "source": [
    "# scaling of neg and pos examples (ratio as in test set) EVEN reduces logloss on training data ~6 percentpoints\n",
    "pos_train = train[train['is_duplicate'] == 1]\n",
    "neg_train = train[train['is_duplicate'] == 0]\n",
    "p = 0.165 # kaggle test set ratio \n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "train2 = pd.concat([pos_train, neg_train])\n",
    "print(train2.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalid-logloss:0.571953\n",
      "Will train until valid-logloss hasn't improved in 20 rounds.\n",
      "[10]\tvalid-logloss:0.393677\n",
      "[20]\tvalid-logloss:0.386118\n",
      "[30]\tvalid-logloss:0.38215\n",
      "[40]\tvalid-logloss:0.380261\n",
      "[50]\tvalid-logloss:0.378966\n",
      "[60]\tvalid-logloss:0.377763\n",
      "[70]\tvalid-logloss:0.377107\n",
      "[80]\tvalid-logloss:0.376484\n",
      "[90]\tvalid-logloss:0.375726\n",
      "[100]\tvalid-logloss:0.3752\n",
      "[110]\tvalid-logloss:0.374837\n",
      "[120]\tvalid-logloss:0.374335\n",
      "[130]\tvalid-logloss:0.373795\n",
      "[140]\tvalid-logloss:0.373538\n",
      "[150]\tvalid-logloss:0.373186\n",
      "[160]\tvalid-logloss:0.372921\n",
      "[170]\tvalid-logloss:0.3727\n",
      "[180]\tvalid-logloss:0.372453\n",
      "[190]\tvalid-logloss:0.372327\n"
     ]
    }
   ],
   "source": [
    "# xgb api with additional methods and params \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train2[col].values, train2[\"is_duplicate\"].values, test_size=0.2, random_state=42)\n",
    "\n",
    "# num_testing = 100000\n",
    "# X_train = train[col].values[:-num_testing]\n",
    "# X_test = train[col].values[-num_testing:]\n",
    "# y_train = train[\"is_duplicate\"].values[:-num_testing]\n",
    "# y_test = train[\"is_duplicate\"].values[-num_testing:]\n",
    "# # classification\n",
    "\n",
    "Dtrain = xgb.DMatrix(X_train,y_train,feature_names=col)\n",
    "Dtest = xgb.DMatrix(X_test,y_test,feature_names=col)\n",
    "\n",
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params['eval_metric'] = 'logloss'\n",
    "params[\"max_depth\"] = 6\n",
    "\n",
    "watchlist = [(Dtest,\"valid\")]\n",
    "\n",
    "bst = xgb.train(params, Dtrain, 200, watchlist, early_stopping_rounds=20, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'Which is the best post graduate medical college to do MS in Orthopaedics?'\n",
      " 'Which is the best private post graduate medical college for MS in orthopaedics?']\n",
      "error: 1.000 with preds 0.000 and truth 1\n",
      "['What are some of the best camping tools?'\n",
      " 'What are some of the best camping blogs?']\n",
      "error: 0.999 with preds 0.001 and truth 1\n",
      "['What is the use of having flavors in the condoms?'\n",
      " 'What is the use of flavoured condoms?']\n",
      "error: 0.999 with preds 0.001 and truth 1\n",
      "['How do I add add text to a IPython/Jupyter Notebook?'\n",
      " 'How do I install IPython Notebook and Sublime Text on the same PC?']\n",
      "error: 0.999 with preds 0.001 and truth 1\n",
      "[\"How did Adam D'Angelo get so much learned people to sign up for Quora?\"\n",
      " 'How did Quora get initial traction?']\n",
      "error: 0.998 with preds 0.002 and truth 1\n",
      "['How do I recover my Gmail email addresses?'\n",
      " 'How do I recover deleted emails in my gmail account?']\n",
      "error: 0.998 with preds 0.002 and truth 1\n",
      "['How you ever been raped?'\n",
      " 'Have you ever been subjected to rape and sexual assault?']\n",
      "error: 0.998 with preds 0.002 and truth 1\n",
      "['Which was your best moment in life?' 'What was your best moment?']\n",
      "error: 0.998 with preds 0.002 and truth 1\n",
      "[ \"If somehow you get to know that you're going to die tomorrow, what would you do today?\"\n",
      " 'If you were to die tomorrow, why not today?']\n",
      "error: 0.998 with preds 0.002 and truth 1\n",
      "['Why do non-parents tell parents how to raise their kids?'\n",
      " 'How can I survive as a vegan in Ukraine?']\n",
      "error: 0.997 with preds 0.003 and truth 1\n",
      "['What is the average yearly salary of an Operations Coordinator at Uber?'\n",
      " 'What does the job of an Operations Coordinator at Uber entail?']\n",
      "error: 0.997 with preds 0.003 and truth 1\n",
      "['Why do sniper rifles have to be reloaded after every shot?'\n",
      " 'What kind of bullets do sniper rifles use?']\n",
      "error: 0.996 with preds 0.004 and truth 1\n",
      "[ 'How does one get selected to be a fighter pilot in Indian air force, what are the difference between cds entries, afcat entries, ncc and nda enties with respect to the above question?'\n",
      " 'How do apply for UES entry of Indian Air Force?']\n",
      "error: 0.996 with preds 0.004 and truth 1\n",
      "[\"What are Hillary Clinton's favorite sports teams and athletes?\"\n",
      " \"What are the Hillary Clinton's favorite sports to watch during personal and family free time?\"]\n",
      "error: 0.995 with preds 0.005 and truth 1\n",
      "['What are some of the best games for Android?'\n",
      " 'What are the most interesting games for Android?']\n",
      "error: 0.995 with preds 0.005 and truth 1\n",
      "['What are gamma rays? Why are they dangerous?'\n",
      " 'When are gamma rays considered dangerous?']\n",
      "error: 0.994 with preds 0.006 and truth 1\n",
      "['Numerology: What, does 12:34 mean I see it all the time?'\n",
      " 'What does it mean when a girl smiles at you every time she sees you?']\n",
      "error: 0.994 with preds 0.006 and truth 1\n",
      "['Sex on the first date. How do I get to the poonani?'\n",
      " 'Is sex on the first date OK?']\n",
      "error: 0.994 with preds 0.006 and truth 1\n",
      "['How to search by license plate?'\n",
      " 'Is it possible to find an owner of a car by license plate?']\n",
      "error: 0.993 with preds 0.007 and truth 1\n",
      "[ \"What are some basic graph theory questions (BFS/DFS/SCC/shortest paths/topo/that sort) from online judges like SPOJ and CodeChef, in increasing order of difficulty? I'm not good at even identifying graph questions.\"\n",
      " 'Where can I get Dynamic Programming questions in increasing order of difficulty starting from very basic?']\n",
      "error: 0.993 with preds 0.007 and truth 1\n"
     ]
    }
   ],
   "source": [
    "preds = bst.predict(Dtest)\n",
    "errors = np.abs(preds-y_test)\n",
    "idx = np.argsort(errors)[::-1] + num_testing\n",
    "\n",
    "for i in range(20):\n",
    "    print(train.iloc[idx[i],3:5].values)\n",
    "    print(\"error: %.3f with preds %.3f and truth %i\" %(errors[idx[i]-num_testing],preds[idx[i]-num_testing],y_test[idx[i]-num_testing]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                            133367\n",
       "qid1                                                          213367\n",
       "qid2                                                          213368\n",
       "question1          Which is the best post graduate medical colleg...\n",
       "question2          Which is the best private post graduate medica...\n",
       "is_duplicate                                                       0\n",
       "question1_nouns          [post, graduate, college, ms, orthopaedics]\n",
       "question2_nouns          [post, graduate, college, ms, orthopaedics]\n",
       "z_len1                                                            73\n",
       "z_len2                                                            79\n",
       "z_word_len1                                                       13\n",
       "z_word_len2                                                       13\n",
       "z_noun_match                                                       5\n",
       "z_match_ratio                                               0.925926\n",
       "z_word_match                                                0.769231\n",
       "Name: 133367, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[idx[0]]\n",
    "#y_test[idx[0]-num_testing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'zip' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-cbb2fdd69caa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'zip' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "zip(y_test.values,preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10.0, 60.0)\n",
    "xgb.plot_importance(bst); plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
