{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timomoeller/.virtualenvs/keras/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def word_match_share(row, stops=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def jaccard(row):\n",
    "    wic = set(row['question1']).intersection(set(row['question2']))\n",
    "    uw = set(row['question1']).union(row['question2'])\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "def common_words(row):\n",
    "    return len(set(row['question1']).intersection(set(row['question2'])))\n",
    "\n",
    "def total_unique_words(row):\n",
    "    return len(set(row['question1']).union(row['question2']))\n",
    "\n",
    "def total_unq_words_stop(row, stops):\n",
    "    return len([x for x in set(row['question1']).union(row['question2']) if x not in stops])\n",
    "\n",
    "def wc_diff(row):\n",
    "    return abs(len(row['question1']) - len(row['question2']))\n",
    "\n",
    "def wc_ratio(row):\n",
    "    l1 = len(row['question1'])*1.0 \n",
    "    l2 = len(row['question2'])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique(row):\n",
    "    return abs(len(set(row['question1'])) - len(set(row['question2'])))\n",
    "\n",
    "def wc_ratio_unique(row):\n",
    "    l1 = len(set(row['question1'])) * 1.0\n",
    "    l2 = len(set(row['question2']))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique_stop(row, stops=None):\n",
    "    return abs(len([x for x in set(row['question1']) if x not in stops]) - len([x for x in set(row['question2']) if x not in stops]))\n",
    "\n",
    "def wc_ratio_unique_stop(row, stops=None):\n",
    "    l1 = len([x for x in set(row['question1']) if x not in stops])*1.0 \n",
    "    l2 = len([x for x in set(row['question2']) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def same_start_word(row):\n",
    "    if not row['question1'] or not row['question2']:\n",
    "        return np.nan\n",
    "    return int(row['question1'][0] == row['question2'][0])\n",
    "\n",
    "def char_diff(row):\n",
    "    return abs(len(''.join(row['question1'])) - len(''.join(row['question2'])))\n",
    "\n",
    "def char_ratio(row):\n",
    "    l1 = len(''.join(row['question1'])) \n",
    "    l2 = len(''.join(row['question2']))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def char_diff_unique_stop(row, stops=None):\n",
    "    return abs(len(''.join([x for x in set(row['question1']) if x not in stops])) - len(''.join([x for x in set(row['question2']) if x not in stops])))\n",
    "\n",
    "\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "    \n",
    "def tfidf_word_match_share_stops(row, stops=None, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def tfidf_word_match_share(row, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "\n",
    "def build_features(data, stops, weights):\n",
    "    X = pd.DataFrame()\n",
    "    f = functools.partial(word_match_share, stops=stops)\n",
    "    X['word_match'] = data.apply(f, axis=1, raw=True) #1\n",
    "\n",
    "    f = functools.partial(tfidf_word_match_share, weights=weights)\n",
    "    X['tfidf_wm'] = data.apply(f, axis=1, raw=True) #2\n",
    "\n",
    "    f = functools.partial(tfidf_word_match_share_stops, stops=stops, weights=weights)\n",
    "    X['tfidf_wm_stops'] = data.apply(f, axis=1, raw=True) #3\n",
    "\n",
    "    X['jaccard'] = data.apply(jaccard, axis=1, raw=True) #4\n",
    "    X['wc_diff'] = data.apply(wc_diff, axis=1, raw=True) #5\n",
    "    X['wc_ratio'] = data.apply(wc_ratio, axis=1, raw=True) #6\n",
    "    X['wc_diff_unique'] = data.apply(wc_diff_unique, axis=1, raw=True) #7\n",
    "    X['wc_ratio_unique'] = data.apply(wc_ratio_unique, axis=1, raw=True) #8\n",
    "\n",
    "    f = functools.partial(wc_diff_unique_stop, stops=stops)    \n",
    "    X['wc_diff_unq_stop'] = data.apply(f, axis=1, raw=True) #9\n",
    "    f = functools.partial(wc_ratio_unique_stop, stops=stops)    \n",
    "    X['wc_ratio_unique_stop'] = data.apply(f, axis=1, raw=True) #10\n",
    "\n",
    "    X['same_start'] = data.apply(same_start_word, axis=1, raw=True) #11\n",
    "    X['char_diff'] = data.apply(char_diff, axis=1, raw=True) #12\n",
    "\n",
    "    f = functools.partial(char_diff_unique_stop, stops=stops) \n",
    "    X['char_diff_unq_stop'] = data.apply(f, axis=1, raw=True) #13\n",
    "\n",
    "#     X['common_words'] = data.apply(common_words, axis=1, raw=True)  #14\n",
    "    X['total_unique_words'] = data.apply(total_unique_words, axis=1, raw=True)  #15\n",
    "\n",
    "    f = functools.partial(total_unq_words_stop, stops=stops)\n",
    "    X['total_unq_words_stop'] = data.apply(f, axis=1, raw=True)  #16\n",
    "    \n",
    "    X['char_ratio'] = data.apply(char_ratio, axis=1, raw=True) #17    \n",
    "\n",
    "    return X\n",
    "\n",
    "def clean_text(text):\n",
    "    #substitute thousands\n",
    "    matches = re.finditer(r'[0-9]+(?P<thousands>\\s{0,2}k\\b)', text, flags=re.I)\n",
    "    result = ''\n",
    "    len_offset = 0\n",
    "    for match in matches:\n",
    "        result += '{}000'.format(text[len(result)-len_offset:match.start('thousands')])\n",
    "        len_offset += 3 - (match.end('thousands') - match.start('thousands'))\n",
    "    result += text[len(result)-len_offset:]\n",
    "    text = result\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"china\", \"China\", text)\n",
    "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"America\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    return text.lower()\n",
    "\n",
    "abbr_dict={\n",
    "    \"what's\":\"what is\",\n",
    "    \"what're\":\"what are\",\n",
    "    \"who's\":\"who is\",\n",
    "    \"who're\":\"who are\",\n",
    "    \"where's\":\"where is\",\n",
    "    \"where're\":\"where are\",\n",
    "    \"when's\":\"when is\",\n",
    "    \"when're\":\"when are\",\n",
    "    \"how's\":\"how is\",\n",
    "    \"how're\":\"how are\",\n",
    "\n",
    "    \"i'm\":\"i am\",\n",
    "    \"we're\":\"we are\",\n",
    "    \"you're\":\"you are\",\n",
    "    \"they're\":\"they are\",\n",
    "    \"it's\":\"it is\",\n",
    "    \"he's\":\"he is\",\n",
    "    \"she's\":\"she is\",\n",
    "    \"that's\":\"that is\",\n",
    "    \"there's\":\"there is\",\n",
    "    \"there're\":\"there are\",\n",
    "\n",
    "    \"i've\":\"i have\",\n",
    "    \"we've\":\"we have\",\n",
    "    \"you've\":\"you have\",\n",
    "    \"they've\":\"they have\",\n",
    "    \"who've\":\"who have\",\n",
    "    \"would've\":\"would have\",\n",
    "    \"not've\":\"not have\",\n",
    "\n",
    "    \"i'll\":\"i will\",\n",
    "    \"we'll\":\"we will\",\n",
    "    \"you'll\":\"you will\",\n",
    "    \"he'll\":\"he will\",\n",
    "    \"she'll\":\"she will\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"they'll\":\"they will\",\n",
    "\n",
    "    \"isn't\":\"is not\",\n",
    "    \"wasn't\":\"was not\",\n",
    "    \"aren't\":\"are not\",\n",
    "    \"weren't\":\"were not\",\n",
    "    \"can't\":\"can not\",\n",
    "    \"couldn't\":\"could not\",\n",
    "    \"don't\":\"do not\",\n",
    "    \"didn't\":\"did not\",\n",
    "    \"shouldn't\":\"should not\",\n",
    "    \"wouldn't\":\"would not\",\n",
    "    \"doesn't\":\"does not\",\n",
    "    \"haven't\":\"have not\",\n",
    "    \"hasn't\":\"has not\",\n",
    "    \"hadn't\":\"had not\",\n",
    "    \"won't\":\"will not\",\n",
    "    '[\"\\'?,\\.]':'',\n",
    "    '\\s+':' '}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train_features.csv', encoding=\"ISO-8859-1\") # abhishek\n",
    "X_train_ab = df_train.iloc[:, 2:-1]\n",
    "X_train_ab = X_train_ab.drop('euclidean_distance', axis=1)\n",
    "X_train_ab = X_train_ab.drop('jaccard_distance', axis=1)\n",
    "\n",
    "df_train = pd.read_csv('../train.csv')\n",
    "df_train = df_train.fillna(' ')\n",
    "df_test = pd.read_csv('../test.csv')\n",
    "df_test = df_test.fillna(' ')\n",
    "\n",
    "# remove class noise\n",
    "df_train = pd.read_csv('../train.csv')\n",
    "df_train = df_train.fillna(' ')\n",
    "df_train.replace(abbr_dict,regex=True,inplace=True)\n",
    "df_train.loc[:,'question1'] = df_train.question1.map(lambda x: x.lower())\n",
    "df_train.loc[:,'question2'] = df_train.question2.map(lambda x: x.lower())\n",
    "\n",
    "idx1 = df_train.is_duplicate == 0\n",
    "idx2 = df_train.question1 == df_train.question2\n",
    "idx3 = (idx1*1 + idx2*1) == 2\n",
    "print(\"%i is duplicate rows switched\" %np.sum(idx3))\n",
    "idxint = np.nonzero(idx3)[0]\n",
    "df_train.iloc[idxint,-1] = 1\n",
    "\n",
    "\n",
    "\n",
    "# clean text\n",
    "df_train.loc[:,'question1'] = df_train.question1.map(lambda x: clean_text(x))\n",
    "df_train.loc[:,'question2'] = df_train.question2.map(lambda x: clean_text(x))\n",
    "\n",
    "\n",
    "df_test.replace(abbr_dict,regex=True,inplace=True)\n",
    "df_test.loc[:,'question1'] = df_test.question1.map(lambda x: clean_text(x))\n",
    "df_test.loc[:,'question2'] = df_test.question2.map(lambda x: clean_text(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ques = pd.concat([df_train[['question1', 'question2']], \\\n",
    "    df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(ques.shape[0]):\n",
    "        q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "        q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "\n",
    "def q1_freq(row):\n",
    "    return(len(q_dict[row['question1']]))\n",
    "\n",
    "def q2_freq(row):\n",
    "    return(len(q_dict[row['question2']]))\n",
    "\n",
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "df_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "df_train['q1_freq'] = df_train.apply(q1_freq, axis=1, raw=True)\n",
    "df_train['q2_freq'] = df_train.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "df_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "df_test['q1_freq'] = df_test.apply(q1_freq, axis=1, raw=True)\n",
    "df_test['q2_freq'] = df_test.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "test_leaky = df_test.loc[:, ['q1_q2_intersect','q1_freq','q2_freq']]\n",
    "#del df_test\n",
    "\n",
    "train_leaky = df_train.loc[:, ['q1_q2_intersect','q1_freq','q2_freq']]\n",
    "\n",
    "# explore\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "df_train['question1'] = df_train['question1'].map(lambda x: str(x).lower().split())\n",
    "df_train['question2'] = df_train['question2'].map(lambda x: str(x).lower().split())\n",
    "\n",
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist())\n",
    "\n",
    "words = [x for y in train_qs for x in y]\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "print('Building Features')\n",
    "X_train = build_features(df_train, stops, weights)\n",
    "X_train = pd.concat((X_train, X_train_ab, train_leaky), axis=1)\n",
    "y_train = df_train['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumped\n"
     ]
    }
   ],
   "source": [
    "pickle.dump([X_train,y_train],open(\"../data/XandytrainForum158cleaned.p\",\"wb\"))\n",
    "print(\"dumped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = pickle.load(open(\"../data/XandytrainForum158.p\",\"rb\"))\n",
    "X_train= temp[0]\n",
    "y_train = temp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.189780815727\n",
      "0.189272444346\n"
     ]
    }
   ],
   "source": [
    "#UPDownSampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=4242)\n",
    "pos_train = X_train[y_train == 1]\n",
    "neg_train = X_train[y_train == 0]\n",
    "X_train = pd.concat((neg_train, pos_train.iloc[:int(0.8*len(pos_train))], neg_train))\n",
    "y_train = np.array([0] * neg_train.shape[0] + [1] * pos_train.iloc[:int(0.8*len(pos_train))].shape[0] + [0] * neg_train.shape[0])\n",
    "print(np.mean(y_train))\n",
    "del pos_train, neg_train\n",
    "\n",
    "pos_valid = X_valid[y_valid == 1]\n",
    "neg_valid = X_valid[y_valid == 0]\n",
    "X_valid = pd.concat((neg_valid, pos_valid.iloc[:int(0.8 * len(pos_valid))], neg_valid))\n",
    "y_valid = np.array([0] * neg_valid.shape[0] + [1] * pos_valid.iloc[:int(0.8 * len(pos_valid))].shape[0] + [0] * neg_valid.shape[0])\n",
    "print(np.mean(y_valid))\n",
    "del pos_valid, neg_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.226481905009\n",
      "0.22590133115\n"
     ]
    }
   ],
   "source": [
    "#UPSampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=4242)\n",
    "pos_train = X_train[y_train == 1]\n",
    "neg_train = X_train[y_train == 0]\n",
    "X_train = pd.concat((neg_train, pos_train, neg_train))\n",
    "y_train = np.array([0] * neg_train.shape[0] + [1] * pos_train.shape[0] + [0] * neg_train.shape[0])\n",
    "print(np.mean(y_train))\n",
    "del pos_train, neg_train\n",
    "\n",
    "pos_valid = X_valid[y_valid == 1]\n",
    "neg_valid = X_valid[y_valid == 0]\n",
    "X_valid = pd.concat((neg_valid, pos_valid, neg_valid))\n",
    "y_valid = np.array([0] * neg_valid.shape[0] + [1] * pos_valid.shape[0] + [0] * neg_valid.shape[0])\n",
    "print(np.mean(y_valid))\n",
    "del pos_valid, neg_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.526197\tvalid-logloss:0.525457\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.300284\tvalid-logloss:0.301808\n",
      "[100]\ttrain-logloss:0.249291\tvalid-logloss:0.252032\n",
      "[150]\ttrain-logloss:0.231175\tvalid-logloss:0.234622\n",
      "[200]\ttrain-logloss:0.222838\tvalid-logloss:0.22698\n",
      "[250]\ttrain-logloss:0.217739\tvalid-logloss:0.222674\n",
      "[300]\ttrain-logloss:0.21338\tvalid-logloss:0.21913\n",
      "[350]\ttrain-logloss:0.20973\tvalid-logloss:0.216531\n",
      "[400]\ttrain-logloss:0.206702\tvalid-logloss:0.214497\n",
      "[450]\ttrain-logloss:0.204056\tvalid-logloss:0.212929\n",
      "[500]\ttrain-logloss:0.201904\tvalid-logloss:0.211776\n",
      "[550]\ttrain-logloss:0.199906\tvalid-logloss:0.210805\n",
      "[600]\ttrain-logloss:0.198135\tvalid-logloss:0.209984\n",
      "[650]\ttrain-logloss:0.196319\tvalid-logloss:0.20923\n",
      "[700]\ttrain-logloss:0.19468\tvalid-logloss:0.208609\n",
      "[750]\ttrain-logloss:0.193117\tvalid-logloss:0.208096\n",
      "[800]\ttrain-logloss:0.191668\tvalid-logloss:0.207602\n",
      "[850]\ttrain-logloss:0.190308\tvalid-logloss:0.207212\n",
      "[900]\ttrain-logloss:0.188947\tvalid-logloss:0.206833\n",
      "[950]\ttrain-logloss:0.187661\tvalid-logloss:0.206524\n",
      "[1000]\ttrain-logloss:0.18635\tvalid-logloss:0.206222\n",
      "[1050]\ttrain-logloss:0.185218\tvalid-logloss:0.205956\n",
      "[1100]\ttrain-logloss:0.183977\tvalid-logloss:0.205661\n",
      "[1150]\ttrain-logloss:0.182822\tvalid-logloss:0.205408\n",
      "[1200]\ttrain-logloss:0.181713\tvalid-logloss:0.20521\n",
      "[1250]\ttrain-logloss:0.180563\tvalid-logloss:0.205032\n",
      "[1300]\ttrain-logloss:0.179441\tvalid-logloss:0.204809\n",
      "[1350]\ttrain-logloss:0.178374\tvalid-logloss:0.204621\n",
      "[1400]\ttrain-logloss:0.17732\tvalid-logloss:0.204401\n",
      "[1450]\ttrain-logloss:0.176317\tvalid-logloss:0.204228\n",
      "[1500]\ttrain-logloss:0.175273\tvalid-logloss:0.204114\n",
      "[1550]\ttrain-logloss:0.174179\tvalid-logloss:0.203999\n",
      "[1600]\ttrain-logloss:0.173217\tvalid-logloss:0.203904\n",
      "[1650]\ttrain-logloss:0.172206\tvalid-logloss:0.203809\n",
      "[1700]\ttrain-logloss:0.171243\tvalid-logloss:0.203727\n",
      "[1750]\ttrain-logloss:0.170289\tvalid-logloss:0.203643\n",
      "[1800]\ttrain-logloss:0.169381\tvalid-logloss:0.203543\n",
      "[1850]\ttrain-logloss:0.168475\tvalid-logloss:0.203514\n",
      "[1900]\ttrain-logloss:0.167558\tvalid-logloss:0.203433\n",
      "[1950]\ttrain-logloss:0.166658\tvalid-logloss:0.203392\n",
      "[2000]\ttrain-logloss:0.165734\tvalid-logloss:0.203331\n",
      "[2050]\ttrain-logloss:0.164828\tvalid-logloss:0.203294\n",
      "[2100]\ttrain-logloss:0.163903\tvalid-logloss:0.2032\n",
      "[2150]\ttrain-logloss:0.162999\tvalid-logloss:0.203146\n",
      "[2200]\ttrain-logloss:0.162182\tvalid-logloss:0.203089\n",
      "[2250]\ttrain-logloss:0.161289\tvalid-logloss:0.203033\n",
      "[2300]\ttrain-logloss:0.160396\tvalid-logloss:0.202973\n",
      "[2350]\ttrain-logloss:0.15952\tvalid-logloss:0.202919\n",
      "[2400]\ttrain-logloss:0.158668\tvalid-logloss:0.202844\n",
      "[2450]\ttrain-logloss:0.15787\tvalid-logloss:0.202782\n",
      "[2500]\ttrain-logloss:0.157038\tvalid-logloss:0.20274\n",
      "0.20274029318\n",
      "dumped\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 7\n",
    "params['subsample'] = 0.6\n",
    "params['base_score'] = 0.19\n",
    "params['nthread'] = 8\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 2501, watchlist, early_stopping_rounds=50, verbose_eval=50)\n",
    "print(log_loss(y_valid, bst.predict(d_valid)))\n",
    "pickle.dump(bst,open(\"../data/XandytrainForum158cleanedBST.p\",\"wb\"))\n",
    "print(\"dumped\")\n",
    "\n",
    "# no text cleaning, no class noise removal, updownsampling, eta 0.02:\n",
    "# [2198]\ttrain-logloss:0.142188\tvalid-logloss:0.182794\n",
    "\n",
    "# text cleaning, class noise removed, updownsampling, eta 0.02:\n",
    "#[2450]\ttrain-logloss:0.142341\tvalid-logloss:0.186482\n",
    "\n",
    "\n",
    "# text cleaning, class noise removed, only upsampling, eta 0.02:\n",
    "\n",
    "#temptest scale pos weight 0.19:\n",
    "#[500]\ttrain-logloss:0.251567\tvalid-logloss:0.258253\n",
    "#without scale pos weight\n",
    "#[500]\ttrain-logloss:0.185092\tvalid-logloss:0.194429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Building Test Features')\n",
    "df_abhifeatures = pd.read_csv('../data/test_features.csv', encoding=\"ISO-8859-1\") # already loaded and cleaned\n",
    "x_test_ab = df_abhifeatures.iloc[:, 2:-1]\n",
    "x_test_ab = x_test_ab.drop('euclidean_distance', axis=1)\n",
    "x_test_ab = x_test_ab.drop('jaccard_distance', axis=1)\n",
    "\n",
    "#df_test = pd.read_csv('../test.csv')\n",
    "#df_test = df_test.fillna(' ')\n",
    "\n",
    "df_test['question1'] = df_test['question1'].map(lambda x: str(x).split())\n",
    "df_test['question2'] = df_test['question2'].map(lambda x: str(x).split())\n",
    "\n",
    "x_test = build_features(df_test, stops, weights)\n",
    "x_test = pd.concat((x_test, x_test_ab, test_leaky), axis=1)\n",
    "\n",
    "\n",
    "pickle.dump(x_test,open(\"../data/XtestForum158cleaned.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "# df1 = pickle.load(open(\"../data/XandytrainForum158.p\",\"rb\"))\n",
    "# df2 = pickle.load(open(\"../data/xandy158untouched.p\",\"rb\")) # they are the same except 2 freq vals, be cause of fillna(\" \")\n",
    "\n",
    "#x_test = pickle.load(open(\"../data/XtestForum158.p\",\"rb\"))\n",
    "#bst = pickle.load(open(\"../data/XandytrainForum158UpdownBST.p\",\"rb\"))\n",
    "#bst = pickle.load(open(\"../data/bstuntouched.p\",\"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertPredictions(preds):\n",
    "    meanTrain = 0.19  #no sampling: 0.37 || only upsampling: 0.226 || upDownsample 0.19\n",
    "    a = 0.165 / meanTrain\n",
    "    b = (1 - 0.165) / (1 - meanTrain)\n",
    "\n",
    "    return (preds * a)  / (preds * a + (1 - preds) * b)\n",
    "\n",
    "\n",
    "d_test = xgb.DMatrix(x_test)\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "p_test_conv = convertPredictions(p_test)\n",
    "\n",
    "df_test = pd.read_csv('../test.csv')\n",
    "df_test[\"is_duplicate\"] = pd.Series(p_test_conv,index=df_test.index)\n",
    "\n",
    "header = [\"test_id\", \"is_duplicate\"]\n",
    "df_test.to_csv('../submissions/predictions.csv', columns = header,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1 = pd.read_csv(\"../submissions/predictions.csv\")\n",
    "p2 = pd.read_csv(\"../submissions/predictionsold.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0829372008725\n",
      "0.0794072414555\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.mean(p1.is_duplicate))\n",
    "print(np.mean(p2.is_duplicate))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
