{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "(404290, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "from keras.utils import np_utils\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv', sep=',', header=0)\n",
    "test_df = pd.read_csv('data/test.csv', sep=',', header=0)\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#removing seemingly useless stopwords decreases performance ~ 1%\n",
    "def removePunktAndStopwords(phrases):\n",
    "    toInclude = set(['above','below','between','but','couldn','didn','doesn','down',\n",
    "                     'few','hadn','haven','isn','just','mightn','mustn','most','needn','nor',\n",
    "                     'not','off','once','only','out','over','should','shouldn','some',\n",
    "                     'very','wasn','weren','won','wouldn','again','against','all',\n",
    "                     'any','aren','you','in','how','what','why','who','will','there'])\n",
    "    stop_wordsLarge = set(stopwords.words('english'))\n",
    "    stop_words = set([w for w in stop_wordsLarge if w not in toInclude])\n",
    "    tokenizer = RegexpTokenizer('[a-z]\\w+')\n",
    "    docs_toke = [tokenizer.tokenize(doc.lower()) for doc in phrases]\n",
    "    stopwordsremoved = [[t for t in temp if t not in stop_words] for temp in docs_toke]\n",
    "    return np.asarray([\" \".join(w) for w in stopwordsremoved])\n",
    "\n",
    "\n",
    "train_df = train_df.fillna(\" \")\n",
    "#shuffle trainset\n",
    "train_df = train_df.sample(frac=1)\n",
    "\n",
    "test_df = test_df.fillna(\" \")\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df.head(2)\n",
    "\n",
    "train_df['question2'] = removePunktAndStopwords(train_df['question2'])\n",
    "train_df['question1'] = removePunktAndStopwords(train_df['question1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rebalance training set\n",
    "#incr. neg. cases by 287% => \n",
    "#select\n",
    "pos_train = train_df[is_duplicate == 1]\n",
    "neg_train = train_df[is_duplicate == 0]\n",
    "\n",
    "# Now we oversample the negative class\n",
    "# There is likely a much more elegant way to do this...\n",
    "p = 0.165\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "x_train = pd.concat([pos_train, neg_train])\n",
    "y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "del pos_train, neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add features\n",
    "import difflib\n",
    "def diff_ratios(st1, st2):\n",
    "    seq = difflib.SequenceMatcher()\n",
    "    seq.set_seqs(str(st1).lower(), str(st2).lower())\n",
    "    return seq.ratio()\n",
    "train_df['z_match_ratio'] = train_df.apply(lambda r: diff_ratios(r.question1, r.question2), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2703f19c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2703f194e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions2 = train_df[\"question2\"].values\n",
    "length2 = np.array([len(x.split()) for x in questions2])\n",
    "questions1 = train_df[\"question1\"].values\n",
    "length1 = np.array([len(x.split()) for x in questions1])\n",
    "plt.figure()\n",
    "\n",
    "#test\n",
    "questions1 = train_df[\"question1\"].values\n",
    "length1 = np.array([len(x.split()) for x in questions1])\n",
    "plt.figure()\n",
    "\n",
    "plt.hist(length1,40,range=[0,55])\n",
    "plt.hist(length2,40,range=[0,55])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Using 59020 word vectors of total vocabulary size: 81601 \n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "\n",
    "toke = Tokenizer()\n",
    "toke.fit_on_texts(np.concatenate([questions1,questions2],axis=0))\n",
    "\n",
    "sequences1 = toke.texts_to_sequences(questions1)\n",
    "sequencesReverse1 = [s[::-1] for s in sequences1]\n",
    "sequences2 = toke.texts_to_sequences(questions2)\n",
    "sequencesReverse2 = [s[::-1] for s in sequences2]\n",
    "\n",
    "\n",
    "word_index = toke.word_index\n",
    "X_train_forward1 = pad_sequences(sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train_reverse1 = pad_sequences(sequencesReverse1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train_forward2 = pad_sequences(sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train_reverse2 = pad_sequences(sequencesReverse2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y_train = train_df[\"is_duplicate\"].values #np_utils.to_categorical()\n",
    "\n",
    "\n",
    "#load glove vectors\n",
    "#download them from http://nlp.stanford.edu/data/glove.6B.zip\n",
    "embeddings_index = {}\n",
    "GLOVE_DIR = '../data/'\n",
    "import os\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.%id.txt' %EMBEDDING_DIM))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "#take only word vecs that are in training dictionary\n",
    "#embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM)) # like that unable to learn unknown wordsembeddings \n",
    "\n",
    "embedding_matrix = np.random.uniform(-0.1,0.1,size=(len(word_index) + 1, EMBEDDING_DIM))\n",
    "found = 0\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        found +=1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Using %i word vectors of total vocabulary size: %i ' %(found,len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/keras/legacy/interfaces.py:86: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=64, kernel_size=5, strides=1, padding=\"valid\")`\n",
      "  '` call to the Keras 2 API: ' + signature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_38 (InputLayer)            (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_39 (InputLayer)            (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)          (None, 20, 300)       24480600                                     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)                (None, 16, 64)        96064                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)             (None, 16, 64)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)             (None, 16, 64)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)               (None, 12, 64)        20544                                        \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalMa (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_34 (InputLayer)            (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_35 (InputLayer)            (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_36 (InputLayer)            (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_37 (InputLayer)            (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)             (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)             (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 300)           19500                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 300)           19500                                        \n",
      "____________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                   (None, 400)           1121600                                      \n",
      "____________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                   (None, 400)           1121600                                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)             (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)             (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)     (None, 800)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)     (None, 800)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 300)           1200                                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, 300)           1200                                         \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)     (None, 2200)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)             (None, 2200)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNor (None, 2200)          8800                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 500)           1100500                                      \n",
      "____________________________________________________________________________________________________\n",
      "p_re_lu_10 (PReLU)               (None, 500)           500                                          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)             (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNor (None, 500)           2000                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 500)           250500                                       \n",
      "____________________________________________________________________________________________________\n",
      "p_re_lu_11 (PReLU)               (None, 500)           500                                          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)             (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNor (None, 500)           2000                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 300)           150300                                       \n",
      "____________________________________________________________________________________________________\n",
      "p_re_lu_12 (PReLU)               (None, 300)           300                                          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)             (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNor (None, 300)           1200                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 1)             301                                          \n",
      "====================================================================================================\n",
      "Total params: 28,398,709.0\n",
      "Trainable params: 3,909,909.0\n",
      "Non-trainable params: 24,488,800.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM with embedding trainable\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, BatchNormalization, Convolution1D, GlobalMaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "opt = Adam(lr=0.001,beta_1=0.5)\n",
    "inp_shape = X_train_forward1.shape[1:]\n",
    "filter_length = 5\n",
    "nb_filter = 64\n",
    "pool_length = 4\n",
    "#MODEL 1: bidirectional LSTM\n",
    "#shared layers\n",
    "shared_embedding = Embedding(len(word_index) + 1,\n",
    "              EMBEDDING_DIM,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=MAX_SEQUENCE_LENGTH,\n",
    "              trainable=False,\n",
    "              input_shape=inp_shape)\n",
    "shared_lstm_forward = LSTM(400, return_sequences=False, dropout=0.35, recurrent_dropout=0.35)\n",
    "shared_lstm_backward = LSTM(400, return_sequences=False, dropout=0.35, recurrent_dropout=0.35)\n",
    "\n",
    "#QUESTION1\n",
    "inpONE_forward = Input(shape=inp_shape)\n",
    "xf = shared_embedding(inpONE_forward)\n",
    "xf = shared_lstm_forward(xf)\n",
    "\n",
    "inpONE_reverse = Input(shape=inp_shape)\n",
    "xr = shared_embedding(inpONE_reverse)\n",
    "xr = shared_lstm_backward(xr)\n",
    "x1 = Concatenate(axis=1)([xf,xr])\n",
    "\n",
    "\n",
    "#QUESTION2\n",
    "inpTWO_forward = Input(shape=inp_shape)\n",
    "xf = shared_embedding(inpTWO_forward)\n",
    "xf = shared_lstm_forward(xf)\n",
    "\n",
    "inpTWO_reverse = Input(shape=inp_shape)\n",
    "xr = shared_embedding(inpTWO_reverse)\n",
    "xr = shared_lstm_backward(xr)\n",
    "x2 = Concatenate(axis=1)([xf,xr])\n",
    "\n",
    "#__________________________________\n",
    "#MODEL 2: CNN\n",
    "#shared: \n",
    "shared_conf1 = Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)\n",
    "shared_conf2 = Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)\n",
    "shared_pooling = GlobalMaxPooling1D()\n",
    "#Q1\n",
    "inp_cnn_1 = Input(shape=inp_shape)\n",
    "cnn_1 = shared_embedding(inp_cnn_1)\n",
    "cnn_1 = shared_conf1(cnn_1)\n",
    "cnn_1 = Dropout(0.2)(cnn_1)\n",
    "\n",
    "cnn_1 = shared_conf2(cnn_1)\n",
    "cnn_1 = shared_pooling(cnn_1)\n",
    "cnn_1 = Dropout(0.2)(cnn_1)\n",
    "\n",
    "cnn_1 = Dense(300)(cnn_1)\n",
    "cnn_1 = Dropout(0.2)(cnn_1)\n",
    "cnn_1 = BatchNormalization()(cnn_1)\n",
    "#Q2\n",
    "inp_cnn_2 = Input(shape=inp_shape)\n",
    "cnn_2 = shared_embedding(inp_cnn_2)\n",
    "cnn_2 = shared_conf1(cnn_2)\n",
    "cnn_2 = Dropout(0.2)(cnn_2)\n",
    "\n",
    "cnn_2 = shared_conf2(cnn_2)\n",
    "cnn_2 = shared_pooling(cnn_2)\n",
    "cnn_2 = Dropout(0.2)(cnn_2)\n",
    "\n",
    "cnn_2 = Dense(300)(cnn_2)\n",
    "cnn_2 = Dropout(0.2)(cnn_2)\n",
    "cnn_2 = BatchNormalization()(cnn_2)\n",
    "\n",
    "#_________________________________\n",
    "# COMBINE\n",
    "x = Concatenate(axis=1)([x1,x2,cnn_1,cnn_2])\n",
    "x = Dropout(0.35)(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(500)(x)\n",
    "x = PReLU()(x)\n",
    "x = Dropout(0.35)(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(500)(x)\n",
    "x = PReLU()(x)\n",
    "x = Dropout(0.35)(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(300)(x)\n",
    "x = PReLU()(x)\n",
    "x = Dropout(0.35)(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "pred = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "model = Model([inpONE_forward,inpONE_reverse,inpTWO_forward,inpTWO_reverse,inp_cnn_1,inp_cnn_2],pred)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['binary_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/5\n",
      "363520/363861 [============================>.] - ETA: 0s - loss: 0.3115 - binary_accuracy: 0.8597Epoch 00000: val_loss improved from inf to 0.37818, saving model to tmp/weights_6.hdf5\n",
      "363861/363861 [==============================] - 302s - loss: 0.3115 - binary_accuracy: 0.8596 - val_loss: 0.3782 - val_binary_accuracy: 0.8212\n",
      "Epoch 2/5\n",
      "363520/363861 [============================>.] - ETA: 0s - loss: 0.3021 - binary_accuracy: 0.8647Epoch 00001: val_loss improved from 0.37818 to 0.37579, saving model to tmp/weights_6.hdf5\n",
      "363861/363861 [==============================] - 303s - loss: 0.3021 - binary_accuracy: 0.8647 - val_loss: 0.3758 - val_binary_accuracy: 0.8250\n",
      "Epoch 3/5\n",
      "363520/363861 [============================>.] - ETA: 0s - loss: 0.2925 - binary_accuracy: 0.8697Epoch 00002: val_loss did not improve\n",
      "363861/363861 [==============================] - 302s - loss: 0.2925 - binary_accuracy: 0.8697 - val_loss: 0.3825 - val_binary_accuracy: 0.8243\n",
      "Epoch 4/5\n",
      "363520/363861 [============================>.] - ETA: 0s - loss: 0.2849 - binary_accuracy: 0.8730Epoch 00003: val_loss did not improve\n",
      "363861/363861 [==============================] - 302s - loss: 0.2849 - binary_accuracy: 0.8730 - val_loss: 0.3858 - val_binary_accuracy: 0.8220\n",
      "Epoch 5/5\n",
      "363520/363861 [============================>.] - ETA: 0s - loss: 0.2776 - binary_accuracy: 0.8769Epoch 00004: val_loss did not improve\n",
      "363861/363861 [==============================] - 301s - loss: 0.2776 - binary_accuracy: 0.8768 - val_loss: 0.3773 - val_binary_accuracy: 0.8297\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath=\"tmp/weights_6.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "losses = model.fit([X_train_forward1,X_train_reverse1,X_train_forward2,X_train_reverse2,X_train_forward1,X_train_forward2],\n",
    "                  y_train,\n",
    "                  epochs=5,\n",
    "                  batch_size=512,\n",
    "                  verbose=1,\n",
    "                  validation_split=0.1,\n",
    "                  callbacks=[checkpointer])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/legend.py:326: UserWarning: Unrecognized location \"lower_left\". Falling back on \"best\"; valid locations are\n",
      "\tbest\n",
      "\tupper right\n",
      "\tupper left\n",
      "\tlower left\n",
      "\tlower right\n",
      "\tright\n",
      "\tcenter left\n",
      "\tcenter right\n",
      "\tlower center\n",
      "\tupper center\n",
      "\tcenter\n",
      "\n",
      "  six.iterkeys(self.codes))))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2651ca81d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys = list(losses.history.keys())\n",
    "for key in keys:\n",
    "    plt.plot(losses.history.get(key),label=key)\n",
    "    \n",
    "plt.legend(loc=\"lower_left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1615872/2345796 [===================>..........] - ETA: 214s"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "#load_model(\"tmp/weights_6.hdf5\")\n",
    "test_df = test_df.fillna(\" \")\n",
    "test_questions1 = test_df[\"question1\"].values\n",
    "test_questions2 = test_df[\"question2\"].values\n",
    "\n",
    "seqsTest1 = toke.texts_to_sequences(test_questions1)\n",
    "seqsTestReverse1 = [s[::-1] for s in seqsTest1]\n",
    "X_test_forward1 = pad_sequences(seqsTest1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test_reverse1 = pad_sequences(seqsTestReverse1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "seqsTest2 = toke.texts_to_sequences(test_questions2)\n",
    "seqsTestReverse2 = [s[::-1] for s in seqsTest2]\n",
    "X_test_forward2 = pad_sequences(seqsTest2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test_reverse2 = pad_sequences(seqsTestReverse2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "preds = model.predict([X_test_forward1,X_test_reverse1,X_test_forward2,X_test_reverse2,X_test_forward1,X_test_forward2],batch_size=512,verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "test_df[\"is_duplicate\"] = pd.Series(preds[:,0],index=test_df.index)\n",
    "header = [\"test_id\", \"is_duplicate\"]\n",
    "test_df.to_csv('submissions/predictions_6.csv', columns = header,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(preds,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.sum(preds<0.5)/preds.shape[0])\n",
    "print(np.sum(y_train == 0)/y_train.shape[0])\n",
    "idx = np.nonzero(preds>0.999)[0]\n",
    "test_df.iloc[idx[:5],1:3].values"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
